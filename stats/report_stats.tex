\documentclass{article}
\usepackage[margin=0.7in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, fancyhdr, color, comment, graphicx, environ}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}
\usepackage{calrsfs}
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}


\renewcommand{\footrulewidth}{0.8pt}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=blue,
}

% fancy style 
\pagestyle{fancy}
% remove fancy style from bottom 
\renewcommand{\footrulewidth}{0pt}

\lhead{Massimiliano Pronesti}
\rhead{Foundations of Statistical Inference} 

\newcommand{\loss}{L(\theta, a)}
\newcommand{\lossRule}{L(\theta, \delta(x))}
\newcommand{\risk}{R(\theta, \delta)}

\begin{document}
	\input{coverPage}
	
	\section{Introduction to Decision Theory}
	We refer to \textbf{statistical decision theory} as the branch of statistics concerned with the problem of making decisions in the presence of statistical knowledge.
	Differently from classical statistics - which is mainly directed towards the use of sample information -, decision statistics takes \textbf{also}\footnote{Sample information is still considered} into account other relevant aspects of the problem, which usually prove crucial in the decision process:
	\begin{itemize}
		\item \textbf{knowledge of possible consequences} of the decision, typically quantified from the loss (or the gain) arising from each possible decision;
		\item \textbf{prior information}, typically arising from past experience of similar situations
	\end{itemize}
	This report introduces the main concepts and tools of statistical decision theory as described in the introduction of \textit{James O Berger. Statistical Decision Theory and Bayesian 
	Analysis. Springer 1985}.
	
	\section{Key elements of decision statistics}
	\subsection{Loss function}
		A key element of the decision theory is the loss function, which defines the result of taking an action $a$ given the unknown quantity $\theta$ affecting the decision process, commonly 
	referred as \textbf{the state of nature} . 
	
	For technical convenience, the loss function satisfies the following chain of inequalities
	\begin{align*}
		\loss \ge -K > - \infty
	\end{align*} 
	assuming, of course, that $\loss$ is defined for all $(\theta, a) \in \Theta \times \mathcal{A}$, being $\Theta$ the parameter space and $\mathcal{A}$ the set of all possible actions . Nevertheless, a critical \textbf{caveat} is that, in many problems, loss function and prior information might not be well defined or even nonunique (at the time of decision making).
	 
	\subsection{Bayesian expected loss}
	In light of the above caveat, the natural way of proceeding consists in considering the \textbf{expected loss} of making a decision and then picking the "optimal" one according to it.

	The \textbf{Bayesian expected loss} refers to the uncertainty in $\theta$, treating it as a random quantity associated to a probability distribution $\pi^*(\theta)$ \footnote{We use $\pi^*$ rather than $\pi$ as the latter one typically refers to the initial prior distribution, whereas the former one is usually the final posterior distribution of $\theta$ (after seeing the data)} 
	\begin{align*}
		\rho(\pi^*, a) = E^{\pi^*} \loss = \int_{\Theta} \loss dF^{\pi^*}(\theta)
	\end{align*}
    
	\subsection{Frequentist risk and its implications}
	The so-called "classical statistics" employs a rather different approach towards loss, i.e. the \textbf{risk function} given a decision rule $\delta(X)$, trying to minimize that:
	\begin{align*}
		\risk = E_{\theta}^{X}[\lossRule] = \int_{\mathcal{X}} \lossRule dF^X (x | \theta) 
	\end{align*}

	Notice that, in a no data problem, $\risk \equiv \loss$. Moreover, differently from Bayesian expected loss - which is a number -, the risk is a function on $\Theta$ which drives us to the following problem: \textbf{being $\theta$ unknown, it's not clear what "small" risk means}. Hence, we need to define a (partial) ordering for decision rules to introduce a choice criteria: given two decision rules $\delta_1, \delta_2$, we say $\delta_1$ is R-better than $\delta_2$ if 
	\begin{align*}
		R(\theta, \delta_1) \leq R(\theta, \delta_2), \ \forall \theta \in \Theta
	\end{align*}
	 with strict inequality holding for some $\theta$. If no R-better decision rule exists, a decision rule $\delta$ is called \textit{admissible} (the definition of \textit{inadmissible} is dual). Nevertheless, the choice of a decision rule is quite more complicated than what we just introduced: there is usually a large class of admissible decision rules for a large problem, which,
	typically, have risk functions being better than the others only locally.
	
	\section{Decision principles}
	This far, we introduced how to perform a statistical analysis. We now describe the major methodologies to actually make a decision: 
	
	\begin{itemize}
		\item \textbf{conditional Bayes decision principle}: choose an action $a \in \mathcal{A}$ such that it minimizes the expected loss $\rho(\pi^*, a)$. Such an action will be called a 
		\textbf{Bayes action} and will be denoted $a^{\pi^*}$. 
	     
		\item \textbf{Bayes risk principle:} as remarked in \textbf{section 2.3}, Bayes risk is a number, hence we define a decision rule minimizing it. Notice that, in a no-data problem, the risk is nothing but the loss which implies that this principle gives the same answer of the conditional Bayes decision one;
		\item \textbf{invariance principle:} if two problems have same formal structure, then same decision must be taken;
		\item\textbf{minimax principle:} this principle is often called for consideration of randomized decision rules. According to it, a rule $\delta^{*M}$ is a \textit{minmax decision rule} if it minimizes $\sup_{\theta} \ R(\theta, \delta^*)$ among all randomized rules in $\mathcal{D}^*$ i.e.
		\begin{align*}
			\sup_{\theta \in \Theta} \ R(\theta, \delta^*) = \inf_{\delta^* \in \mathcal{D}^*} \sup_{\theta \in \Theta} \ R(\theta, \delta^*)
		\end{align*}
	\end{itemize}
	
	\section{Foundations}
	In this last section, we're gonna expand even further the conditional versus frequentist controversy - crucial in statistics - as well as discuss some incorrect usages of classical inference in decision problems.
	
	\subsection{Misused inference procedures}	
	A common mistake while analyzing the null hypothesis is to run tests on it without getting deep into its purposes, with the consequence of often discarding a "useful" hypothesis. An interesting example
	involves \textbf{Kepler's law} of planetary motion, which is in fact an approximation of the reality (like every physics model). Nonetheless, if blindly statistically tested with too accurate data, such a rather essentially correct model would be rejected. In other words, a \textbf{statistically significant difference} between the true model and the null hypothesis might be \textbf{unimportant practically}.  
	\subsection{Frequentist Perspective}
	The frequentist approach aims at producing measures which don't depend upon $\theta$, or any prior knowledge about it. It considers a procedure $\delta(x)$ and a criterion function $L(\theta, \delta(x))$ to determine a quantity $\bar{R}$ s.t. repeated use of $\delta$ in different problems would yield average the long run performance of at least $\bar{R}$.

	
	\subsection{Conditional Perspective}
	The conditional approach mainly focuses on the performance of a procedure $\delta(x)$  on \textit{actual} data while the overall performance is considered almost secondary. This  might definitely lead to different results with respect to the frequentist approach introduced above. In order to distinguish between
	those two categories of results, Savage (1962) introduced the terms \textit{initial precision} and \textit{final precision} to, respectively, refer to frequentist and conditional measures. In fact, before seeing the data we can only use frequentist analysis, while after observing it we can make a more precise one.
	

	\subsection{Likelihood Principle}
	This principle makes explicit the natural conditional idea that only the actual
	observed x should be relevant to conclusions or evidence about $\theta$. The key concept in the Likelihood Principle is that of the likelihood function $l(\theta) = f(x | \theta)$ which, intuitively, takes its name from the fact that an input $\theta$ for which $l(\theta)$ is large is more "likely" to be the true $\theta$ than a $\theta$ for which $l(\theta)$ is small.
	Notice that two likelihood values $l_1(\theta), l_2(\theta)$ contain the same information about $\theta$ if they are proportional\footnote{It's implicit that $l_1, l_2$ must refer to the same $\theta$}.
	
	It is important to highlight, however, that the likelihood function contains \textbf{all experimental information} about $\theta$,\textbf{ but not all the information in general}: there could be some other important statistical information, as the
	prior information or considerations of loss. 
	
	\textbf{Limitations:} this principle is not
	enough to decide, at a given stage of our analysis, whether or not to take another observation.
	Besides, it is not enough to predict a future value of X and does not indicate how the likelihood function should be used to make decisions or inferences about $\theta$.
	
\end{document}