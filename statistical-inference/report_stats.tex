\documentclass{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, fancyhdr, color, comment, graphicx, environ}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
%\usepackage{indentfirst}
\usepackage{hyperref}
\usepackage{calrsfs}
\DeclareMathAlphabet{\pazocal}{OMS}{zplm}{m}{n}


\renewcommand{\footrulewidth}{0.8pt}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=blue,
}

% fancy style 
\pagestyle{fancy}
% remove fancy style from bottom 
\renewcommand{\footrulewidth}{0pt}

\lhead{Massimiliano Pronesti}
\rhead{Foundations of Statistical Inference} 

\newcommand{\loss}{L(\theta, a)}
\newcommand{\lossRule}{L(\theta, \delta(x))}
\newcommand{\risk}{R(\theta, \delta)}

\begin{document}
	\input{coverPage}
	
	\section{Introduction to Decision Theory}
	We refer to \textbf{statistical decision theory} as the branch of statistics concerned with the problem of making decisions in the presence of stastistical knowledge.
	
	Differently from classical statistics - which is mainly directed towards the use of sample information -, decision statistics takes \textbf{also} (sample information is still considered) into account other relevant aspects of the problem which usually prove crucial in the decision process.
	These can be grouped in two categories:
	\begin{itemize}
		\item \textbf{knowledge of possible consequences} of the decision, tipically quantified from the loss (or the gain) arising from each possible decision;
		\item \textbf{prior information}, tipically arising from past experience of similar situations
	\end{itemize}
	
	\section{Key elements of decision statistics}
	\subsection{Loss function}
		As anticiped in section 1, a key element of the decision theory is the loss function, which defines the result of taking an action $a$ given the unknown quantity $\theta$ affecting the decision process, commonly 
	referred as \textbf{the state of nature} . 
	
	For technical convenience, the loss function satisfies the following chain of inequalities
	\begin{align*}
		\loss \ge -K > - \infty
	\end{align*} 
	assuming, of course, that $\loss$ is defined for all $(\theta, a) \in \Theta \times \mathcal{A}$, being $\Theta$ the parameter space and $\mathcal{A}$ the set of all possible actions . Nevertheless, a critical \textbf{caveat} is that, in many problems, loss function and prior information might not be well defined or even nonunique (at the time of decision making).
	 
	\subsection{Bayesian expected loss}
	In light of the above caveat, the natural way of proceeding consists in considering the \textbf{expected loss} of making a decision and then picking the "optimal" one according to it.
	
	The \textbf{Bayesian expected loss} refers to the uncertainty in $\theta$, treating it as a random quantity associated to a probability distribution $\pi^*(\theta)$ \footnote{We use $\pi^*$ rather than $\pi$ as the latter one typically refers to the initial prior distribution, while the former one is usually the final posterior distribution of $\theta$ (after seeing the data)} 
	\begin{align*}
		\rho(\pi^*, a) = E^{\pi^*} \loss = \int_{\Theta} \loss dF^{\pi^*}(\theta)
	\end{align*}
    
	\subsection{Frequentist risk and its implications}
	The so-called "classical statistics" employs a rather different approach towards loss, i.e. the \textbf{risk function} given a decision rule $\delta(X)$, trying to minimize that:
	\begin{align*}
		\risk = E_{\theta}^{X}[\lossRule] = \int_{\mathcal{A}} \lossRule dF^X (x | \theta) 
	\end{align*}

	It is important to highlight that in a no data problem, $\risk \equiv \loss$. Moreover, differently from Bayesian expected loss - which is a number -, the risk is a function on $\Theta$ which drives us to the following problem: \textbf{being $\theta$ unknown, it's not clear what "small" risk means}.
	
	We need to define a (partial) ordering for decision rules to introduce a choice criteria: given two decision rules $\delta_1, \delta_2$, we say $\delta_1$ is R-better than $\delta_2$ if 
	\begin{align*}
		R(\theta, \delta_1) \leq R(\theta, \delta_2) \ \forall \theta \in \Theta
	\end{align*}
	 whith strict inequality holding for some $\theta$. If no R-better decision rule exists, a decision rule $\delta$ is called \textit{admissible} (the definition of \textit{inadmissible} is dual).
	
	Nevertheless, there is usually a large class of admissible decision rules for a large problem, which,
	typycally, have risk functions being better than the others only locally.
	\section{Decision principles}
	This far, we introduced how to perform a statistical analisys. We now describe the major methodologies to actually make a decision. 
	\subsection{Conditional Bayes Decision Principle}
	Let $a \in \mathcal{A}$ be an action minimizing the expected loss $\rho(pi^*, a)$. Such an action will be called a \textbf{Bayes action} an will be denoted $a^{\pi^*}$. 
	\subsection{Frequentist Decision Principles}
	Another possibility to select a decision rule is employing the risk function, although this proves more  effortful because of the multiple decision rules admissible. In decision theroy, we can identify three main principles that can be used to develop statistical procedures:
	\begin{itemize}
		\item \textbf{Bayes risk principle:} as remarked in \textbf{section 2.3}, Bayes risk is a number, hence we define a decision rule minimizing it. Notice that, in a no-data problem, the risk is just the loss which implies that this principle gives the same answer of the conditional Bayes decision one.
		\item\textbf{minimax principle:} this principle is often called for consideration of randomized decision rules. According to it, given two randomized decision rules $\delta_1^*, \delta_2^* \in \mathcal{D^*}$, the first is preferred to the second if
		\begin{align*}
			\sup_{\theta \in \Theta} \ R(\theta, \delta_1^*) < 	\sup_{\theta \in \Theta} \ R(\theta, \delta_2^*)
		\end{align*} 
		\item \textbf{invariance principle:} if two problems have same formal structure, then same decision must be taken
	\end{itemize}
	\section{Foundations}
	In this last section, we're gonna expand even further the conditional versus frequentist controversy - crucial in statistics - as well as discuss some incorrect usages of classical inference in decision problems.
	\subsection{Misused inference procedures}
	\subsection{Frequentist Perspective}
	\subsection{Conditional Perspective}
	\subsection{Likelihood Principle}
\end{document}