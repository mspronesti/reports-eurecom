
@article{Buffoni2021-jg,
  title         = "New Trends in Quantum Machine Learning",
  author        = "Buffoni, Lorenzo and Caruso, Filippo",
  abstract      = "Here we will give a perspective on new possible interplays
                   between Machine Learning and Quantum Physics, including also
                   practical cases and applications. We will explore the ways
                   in which machine learning could benefit from new quantum
                   technologies and algorithms to find new ways to speed up
                   their computations by breakthroughs in physical hardware, as
                   well as to improve existing models or devise new learning
                   schemes in the quantum domain. Moreover, there are lots of
                   experiments in quantum physics that do generate incredible
                   amounts of data and machine learning would be a great tool
                   to analyze those and make predictions, or even control the
                   experiment itself. On top of that, data visualization
                   techniques and other schemes borrowed from machine learning
                   can be of great use to theoreticians to have better
                   intuition on the structure of complex manifolds or to make
                   predictions on theoretical models. This new research field,
                   named as Quantum Machine Learning, is very rapidly growing
                   since it is expected to provide huge advantages over its
                   classical counterpart and deeper investigations are timely
                   needed since they can be already tested on the already
                   commercially available quantum machines.",
  month         =  aug,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "2108.09664"
}


@article{Biamonte2016-rb,
  title         = "Quantum Machine Learning",
  author        = "Biamonte, Jacob and Wittek, Peter and Pancotti, Nicola and
                   Rebentrost, Patrick and Wiebe, Nathan and Lloyd, Seth",
  abstract      = "Fuelled by increasing computer power and algorithmic
                   advances, machine learning techniques have become powerful
                   tools for finding patterns in data. Since quantum systems
                   produce counter-intuitive patterns believed not to be
                   efficiently produced by classical systems, it is reasonable
                   to postulate that quantum computers may outperform classical
                   computers on machine learning tasks. The field of quantum
                   machine learning explores how to devise and implement
                   concrete quantum software that offers such advantages.
                   Recent work has made clear that the hardware and software
                   challenges are still considerable but has also opened paths
                   towards solutions.",
  month         =  nov,
  year          =  2016,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "1611.09347"
}


@article{phillipson,
  author =       "Frank Phillipson",
  title =        "Quantum Machine Learning: Benefits and Practical Examples",
  year =         "2020"
}

@article{zhang,
  author =       "Yao Zhang and Qiang Ni",
  title =        "Recent Advances in Quantum Machine Learning",
  DOI =          "https://doi.org/10.1002/que2.34",
  year =         "2020"
  }

@article{Pennylane,
    author = "Ville Bergholm and Josh Izaac and Maria Schuld and Christian Gogolin and Carsten Blank               and Keri McKiernan and Nathan Killoran",
    year = "2018",
    title = "arXiv:1811.04968"
}

@misc{ Qiskit,
       author = {MD SAJID ANIS and H{\'e}ctor Abraham and AduOffei and Rochisha Agarwal and Gabriele Agliardi and Merav Aharoni and Ismail Yunus Akhalwaya and Gadi Aleksandrowicz and Thomas Alexander and Matthew Amy and Sashwat Anagolum and Eli Arbel and Abraham Asfaw and Anish Athalye and Artur Avkhadiev and Carlos Azaustre and PRATHAMESH BHOLE and Abhik Banerjee and Santanu Banerjee and Will Bang and Aman Bansal and Panagiotis Barkoutsos and Ashish Barnawal and George Barron and George S. Barron and Luciano Bello and Yael Ben-Haim and M. Chandler Bennett and Daniel Bevenius and Dhruv Bhatnagar and Arjun Bhobe and Paolo Bianchini and Lev S. Bishop and Carsten Blank and Sorin Bolos and Soham Bopardikar and Samuel Bosch and Sebastian Brandhofer and Brandon and Sergey Bravyi and Nick Bronn and Bryce-Fuller and David Bucher and Artemiy Burov and Fran Cabrera and Padraic Calpin and Lauren Capelluto and Jorge Carballo and Gin{\'e}s Carrascal and Adam Carriker and Ivan Carvalho and Adrian Chen and Chun-Fu Chen and Edward Chen and Jielun (Chris) Chen and Richard Chen and Franck Chevallier and Kartik Chinda and Rathish Cholarajan and Jerry M. Chow and Spencer Churchill and CisterMoke and Christian Claus and Christian Clauss and Caleb Clothier and Romilly Cocking and Ryan Cocuzzo and Jordan Connor and Filipe Correa and Abigail J. Cross and Andrew W. Cross and Simon Cross and Juan Cruz-Benito and Chris Culver and Antonio D. C{\'o}rcoles-Gonzales and Navaneeth D and Sean Dague and Tareq El Dandachi and Animesh N Dangwal and Jonathan Daniel and Marcus Daniels and Matthieu Dartiailh and Abd{\'o}n Rodr{\'\i}guez Davila and Faisal Debouni and Anton Dekusar and Amol Deshmukh and Mohit Deshpande and Delton Ding and Jun Doi and Eli M. Dow and Eric Drechsler and Eugene Dumitrescu and Karel Dumon and Ivan Duran and Kareem EL-Safty and Eric Eastman and Grant Eberle and Amir Ebrahimi and Pieter Eendebak and Daniel Egger and ElePT and Emilio and Alberto Espiricueta and Mark Everitt and Davide Facoetti and Farida and Paco Mart{\'\i}n Fern{\'a}ndez and Samuele Ferracin and Davide Ferrari and Axel Hern{\'a}ndez Ferrera and Romain Fouilland and Albert Frisch and Andreas Fuhrer and Bryce Fuller and MELVIN GEORGE and Julien Gacon and Borja Godoy Gago and Claudio Gambella and Jay M. Gambetta and Adhisha Gammanpila and Luis Garcia and Tanya Garg and Shelly Garion and James R. Garrison and Tim Gates and Leron Gil and Austin Gilliam and Aditya Giridharan and Juan Gomez-Mosquera and Gonzalo and Salvador de la Puente Gonz{\'a}lez and Jesse Gorzinski and Ian Gould and Donny Greenberg and Dmitry Grinko and Wen Guan and Dani Guijo and John A. Gunnels and Harshit Gupta and Naman Gupta and Jakob M. G{\"u}nther and Mikael Haglund and Isabel Haide and Ikko Hamamura and Omar Costa Hamido and Frank Harkins and Kevin Hartman and Areeq Hasan and Vojtech Havlicek and Joe Hellmers and {\L}ukasz Herok and Stefan Hillmich and Hiroshi Horii and Connor Howington and Shaohan Hu and Wei Hu and Junye Huang and Rolf Huisman and Haruki Imai and Takashi Imamichi and Kazuaki Ishizaki and Ishwor and Raban Iten and Toshinari Itoko and Alexander Ivrii and Ali Javadi and Ali Javadi-Abhari and Wahaj Javed and Qian Jianhua and Madhav Jivrajani and Kiran Johns and Scott Johnstun and Jonathan-Shoemaker and JosDenmark and JoshDumo and John Judge and Tal Kachmann and Akshay Kale and Naoki Kanazawa and Jessica Kane and Kang-Bae and Annanay Kapila and Anton Karazeev and Paul Kassebaum and Josh Kelso and Scott Kelso and Vismai Khanderao and Spencer King and Yuri Kobayashi and Kovi11Day and Arseny Kovyrshin and Rajiv Krishnakumar and Vivek Krishnan and Kevin Krsulich and Prasad Kumkar and Gawel Kus and Ryan LaRose and Enrique Lacal and Rapha{\"e}l Lambert and Haggai Landa and John Lapeyre and Joe Latone and Scott Lawrence and Christina Lee and Gushu Li and Jake Lishman and Dennis Liu and Peng Liu and Liam Madden and Yunho Maeng and Saurav Maheshkar and Kahan Majmudar and Aleksei Malyshev and Mohamed El Mandouh and Joshua Manela and Manjula and Jakub Marecek and Manoel Marques and Kunal Marwaha and Dmitri Maslov and Pawe{\l} Maszota and Dolph Mathews and Atsushi Matsuo and Farai Mazhandu and Doug McClure and Maureen McElaney and Cameron McGarry and David McKay and Dan McPherson and Srujan Meesala and Dekel Meirom and Corey Mendell and Thomas Metcalfe and Martin Mevissen and Andrew Meyer and Antonio Mezzacapo and Rohit Midha and Daniel Miller and Zlatko Minev and Abby Mitchell and Nikolaj Moll and Alejandro Montanez and Gabriel Monteiro and Michael Duane Mooring and Renier Morales and Niall Moran and David Morcuende and Seif Mostafa and Mario Motta and Romain Moyard and Prakash Murali and Jan M{\"u}ggenburg and Tristan NEMOZ and David Nadlinger and Ken Nakanishi and Giacomo Nannicini and Paul Nation and Edwin Navarro and Yehuda Naveh and Scott Wyman Neagle and Patrick Neuweiler and Aziz Ngoueya and Johan Nicander and Nick-Singstock and Pradeep Niroula and Hassi Norlen and NuoWenLei and Lee James O'Riordan and Oluwatobi Ogunbayo and Pauline Ollitrault and Tamiya Onodera and Raul Otaolea and Steven Oud and Dan Padilha and Hanhee Paik and Soham Pal and Yuchen Pang and Ashish Panigrahi and Vincent R. Pascuzzi and Simone Perriello and Eric Peterson and Anna Phan and Francesco Piro and Marco Pistoia and Christophe Piveteau and Julia Plewa and Pierre Pocreau and Alejandro Pozas-Kerstjens and Rafa{\l} Pracht and Milos Prokop and Viktor Prutyanov and Sumit Puri and Daniel Puzzuoli and Jes{\'u}s P{\'e}rez and Quant02 and Quintiii and Isha R and Rafey Iqbal Rahman and Arun Raja and Roshan Rajeev and Nipun Ramagiri and Anirudh Rao and Rudy Raymond and Oliver Reardon-Smith and Rafael Mart{\'\i}n-Cuevas Redondo and Max Reuter and Julia Rice and Matt Riedemann and Rietesh and Drew Risinger and Marcello La Rocca and Diego M. Rodr{\'\i}guez and RohithKarur and Ben Rosand and Max Rossmannek and Mingi Ryu and Tharrmashastha SAPV and Nahum Rosa Cruz Sa and Arijit Saha and Abdullah Ash- Saki and Sankalp Sanand and Martin Sandberg and Hirmay Sandesara and Ritvik Sapra and Hayk Sargsyan and Aniruddha Sarkar and Ninad Sathaye and Bruno Schmitt and Chris Schnabel and Zachary Schoenfeld and Travis L. Scholten and Eddie Schoute and Mark Schulterbrandt and Joachim Schwarm and James Seaward and Sergi and Ismael Faro Sertage and Kanav Setia and Freya Shah and Nathan Shammah and Rohan Sharma and Yunong Shi and Jonathan Shoemaker and Adenilton Silva and Andrea Simonetto and Deeksha Singh and Divyanshu Singh and Parmeet Singh and Phattharaporn Singkanipa and Yukio Siraichi and Siri and Jes{\'u}s Sistos and Iskandar Sitdikov and Seyon Sivarajah and Magnus Berg Sletfjerding and John A. Smolin and Mathias Soeken and Igor Olegovich Sokolov and Igor Sokolov and Vicente P. Soloviev and SooluThomas and Starfish and Dominik Steenken and Matt Stypulkoski and Adrien Suau and Shaojun Sun and Kevin J. Sung and Makoto Suwama and Oskar S{\l}owik and Hitomi Takahashi and Tanvesh Takawale and Ivano Tavernelli and Charles Taylor and Pete Taylour and Soolu Thomas and Kevin Tian and Mathieu Tillet and Maddy Tod and Miroslav Tomasik and Caroline Tornow and Enrique de la Torre and Juan Luis S{\'a}nchez Toural and Kenso Trabing and Matthew Treinish and Dimitar Trenev and TrishaPe and Felix Truger and Georgios Tsilimigkounakis and Davindra Tulsi and Wes Turner and Yotam Vaknin and Carmen Recio Valcarce and Francois Varchon and Adish Vartak and Almudena Carrera Vazquez and Prajjwal Vijaywargiya and Victor Villar and Bhargav Vishnu and Desiree Vogt-Lee and Christophe Vuillot and James Weaver and Johannes Weidenfeller and Rafal Wieczorek and Jonathan A. Wildstrom and Jessica Wilson and Erick Winston and WinterSoldier and Jack J. Woehr and Stefan Woerner and Ryan Woo and Christopher J. Wood and Ryan Wood and Steve Wood and James Wootton and Matt Wright and Lucy Xing and Jintao YU and Bo Yang and Daniyar Yeralin and Ryota Yonekura and David Yonge-Mallo and Ryuhei Yoshida and Richard Young and Jessie Yu and Lebin Yu and Christopher Zachow and Laura Zdanski and Helena Zhang and Iulia Zidaru and Christa Zoufal and aeddins-ibm and alexzhang13 and b63 and bartek-bartlomiej and bcamorrison and brandhsn and charmerDark and deeplokhande and dekel.meirom and dime10 and dlasecki and ehchen and fanizzamarco and fs1132429 and gadial and galeinston and georgezhou20 and georgios-ts and gruu and hhorii and hykavitha and itoko and jessica-angel7 and jezerjojo14 and jliu45 and jscott2 and klinvill and krutik2966 and ma5x and michelle4654 and msuwama and ntgiwsvp and ordmoj and sagar pahwa and pritamsinha2304 and ryancocuzzo and saswati-qiskit and septembrr and sethmerkel and shaashwat and sternparky and strickroman and tigerjack and tsura-crisaldo and vadebayo49 and welien and willhbang and wmurphy-collabstar and yang.luh and Mantas {\v{C}}epulkovskis},
       title = {Qiskit: An Open-source Framework for Quantum Computing},
       year = {2021},
       doi = {10.5281/zenodo.2573505}
}

@online{qknn,
    author = "Daniël Kok",
    title = "Qiskit Quantum kNN: A pure quantum knn classifier for a gated quantum computer",
    url  = "https://qiskit-quantum-knn.readthedocs.io"
}
@inproceedings{10.5555/1283383.1283494,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {K-Means++: The Advantages of Careful Seeding},
year = {2007},
isbn = {9780898716245},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1027–1035},
numpages = {9},
location = {New Orleans, Louisiana},
series = {SODA '07}
}
@inproceedings{10.1145/1137856.1137880,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {How Slow is the k-Means Method?},
year = {2006},
isbn = {1595933409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137856.1137880},
doi = {10.1145/1137856.1137880},
abstract = {The k-means method is an old but popular clustering algorithm known for its observed speed and its simplicity. Until recently, however, no meaningful theoretical bounds were known on its running time. In this paper, we demonstrate that the worst-case running time of k-means is superpolynomial by improving the best known lower bound from Ω(n) iterations to 2Ω(√n).},
booktitle = {Proceedings of the Twenty-Second Annual Symposium on Computational Geometry},
pages = {144–153},
numpages = {10},
keywords = {k-means, local search, lower bounds},
location = {Sedona, Arizona, USA},
series = {SCG '06}
}
@ARTICLE{Kerenidis2018-gl,
  title         = "q-means: A quantum algorithm for unsupervised machine
                   learning",
  author        = "Kerenidis, Iordanis and Landman, Jonas and Luongo,
                   Alessandro and Prakash, Anupam",
  abstract      = "Quantum machine learning is one of the most promising
                   applications of a full-scale quantum computer. Over the past
                   few years, many quantum machine learning algorithms have
                   been proposed that can potentially offer considerable
                   speedups over the corresponding classical algorithms. In
                   this paper, we introduce q-means, a new quantum algorithm
                   for clustering which is a canonical problem in unsupervised
                   machine learning. The $q$-means algorithm has convergence
                   and precision guarantees similar to $k$-means, and it
                   outputs with high probability a good approximation of the
                   $k$ cluster centroids like the classical algorithm. Given a
                   dataset of $N$ $d$-dimensional vectors $v_i$ (seen as a
                   matrix $V \in \mathbb\{R\}^\{N \times d\})$ stored in QRAM,
                   the running time of q-means is $\widetilde\{O\}\left( k d
                   \frac\{\eta\}\{\delta^2\}\kappa(V)(\mu(V) + k
                   \frac\{\eta\}\{\delta\}) + k^2
                   \frac\{\eta^\{1.5\}\}\{\delta^2\} \kappa(V)\mu(V) \right)$
                   per iteration, where $\kappa(V)$ is the condition number,
                   $\mu(V)$ is a parameter that appears in quantum linear
                   algebra procedures and $\eta = \max_\{i\}
                   ||v_\{i\}||^\{2\}$. For a natural notion of well-clusterable
                   datasets, the running time becomes $\widetilde\{O\}\left(
                   k^2 d \frac\{\eta^\{2.5\}\}\{\delta^3\} + k^\{2.5\}
                   \frac\{\eta^2\}\{\delta^3\} \right)$ per iteration, which is
                   linear in the number of features $d$, and polynomial in the
                   rank $k$, the maximum square norm $\eta$ and the error
                   parameter $\delta$. Both running times are only
                   polylogarithmic in the number of datapoints $N$. Our
                   algorithm provides substantial savings compared to the
                   classical $k$-means algorithm that runs in time $O(kdN)$ per
                   iteration, particularly for the case of large datasets.",
  month         =  dec,
  year          =  2018,
  copyright     = "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "1812.03584"
}
@ARTICLE{Li2021-le,
  title         = "Quantum K-nearest neighbor classification algorithm based on
                   Hamming distance",
  author        = "Li, Jing and Lin, Song and Kai, Yu and Guo, Gongde",
  abstract      = "K-nearest neighbor classification algorithm is one of the
                   most basic algorithms in machine learning, which determines
                   the sample's category by the similarity between samples. In
                   this paper, we propose a quantum K-nearest neighbor
                   classification algorithm with Hamming distance. In this
                   algorithm, quantum computation is firstly utilized to obtain
                   Hamming distance in parallel. Then, a core sub-algorithm for
                   searching the minimum of unordered integer sequence is
                   presented to find out the minimum distance. Based on these
                   two sub-algorithms, the whole quantum frame of K-nearest
                   neighbor classification algorithm is presented. At last, it
                   is shown that the proposed algorithm can achieve a
                   quadratical speedup by analyzing its time complexity
                   briefly.",
  month         =  mar,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "2103.04253"
}
@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}
@article{data_encoding,
title ="Data Encoding Patterns for Quantum Computing",
author = {MANUELA WEIGOLD and OHANNA BARZEN and RANK LEYMANN and ARIE SALM},
year = {2020},
url = "https://hillside.net/plop/2020/papers/weigold.pdf"

}
@online{metrics,
title = "A demo of K-Means clustering on the handwritten digits data",
url = "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html"}
@ARTICLE{Basheer2020-ww,
  title         = "Quantum $k$-nearest neighbors algorithm",
  author        = "Basheer, Afrad and Afham, A and Goyal, Sandeep K",
  abstract      = "One of the simplest and most effective classical machine
                   learning algorithms is the $k$-nearest neighbors algorithm
                   ($k$NN) which classifies an unknown test state by finding
                   the $k$ nearest neighbors from a set of $M$ train states.
                   Here we present a quantum analog of classical $k$NN $-$
                   quantum $k$NN (Q$k$NN) $-$ based on fidelity as the
                   similarity measure. We show that Q$k$NN algorithm can be
                   reduced to an instance of the quantum $k$-maxima algorithm,
                   hence the query complexity of Q$k$NN is $O(\sqrt\{kM\})$.
                   The non-trivial task in this reduction is to encode the
                   fidelity information between the test state and all the
                   train states as amplitudes of a quantum state. Converting
                   this amplitude encoded information to a digital format
                   enables us to compare them efficiently, thus completing the
                   reduction. Unlike classical $k$NN and existing quantum $k$NN
                   algorithms, the proposed algorithm can be directly used on
                   quantum data thereby bypassing expensive processes such as
                   quantum state tomography. As an example, we show the
                   applicability of this algorithm in entanglement
                   classification and quantum state discrimination.",
  month         =  mar,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "2003.09187"
}
@article{svm_inversion,
author = {Suykens, Johan and Vandewalle, Joos},
year = {1999},
month = {06},
pages = {293-300},
title = {Least Squares Support Vector Machine Classifiers},
volume = {9},
journal = {Neural Processing Letters},
doi = {10.1023/A:1018628609742}
}
@ARTICLE{Rebentrost2013-ar,
  title         = "Quantum support vector machine for big data classification",
  author        = "Rebentrost, Patrick and Mohseni, Masoud and Lloyd, Seth",
  abstract      = "Supervised machine learning is the classification of new
                   data based on already classified training examples. In this
                   work, we show that the support vector machine, an optimized
                   binary classifier, can be implemented on a quantum computer,
                   with complexity logarithmic in the size of the vectors and
                   the number of training examples. In cases when classical
                   sampling algorithms require polynomial time, an exponential
                   speed-up is obtained. At the core of this quantum big data
                   algorithm is a non-sparse matrix exponentiation technique
                   for efficiently performing a matrix inversion of the
                   training data inner-product (kernel) matrix.",
  month         =  jul,
  year          =  2013,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "1307.0471"
}
@ARTICLE{Havlicek2018-hi,
  title         = "Supervised learning with quantum enhanced feature spaces",
  author        = "Havlicek, Vojtech and C{\'o}rcoles, Antonio D and Temme,
                   Kristan and Harrow, Aram W and Kandala, Abhinav and Chow,
                   Jerry M and Gambetta, Jay M",
  abstract      = "Machine learning and quantum computing are two technologies
                   each with the potential for altering how computation is
                   performed to address previously untenable problems. Kernel
                   methods for machine learning are ubiquitous for pattern
                   recognition, with support vector machines (SVMs) being the
                   most well-known method for classification problems. However,
                   there are limitations to the successful solution to such
                   problems when the feature space becomes large, and the
                   kernel functions become computationally expensive to
                   estimate. A core element to computational speed-ups afforded
                   by quantum algorithms is the exploitation of an
                   exponentially large quantum state space through controllable
                   entanglement and interference. Here, we propose and
                   experimentally implement two novel methods on a
                   superconducting processor. Both methods represent the
                   feature space of a classification problem by a quantum
                   state, taking advantage of the large dimensionality of
                   quantum Hilbert space to obtain an enhanced solution. One
                   method, the quantum variational classifier builds on [1,2]
                   and operates through using a variational quantum circuit to
                   classify a training set in direct analogy to conventional
                   SVMs. In the second, a quantum kernel estimator, we estimate
                   the kernel function and optimize the classifier directly.
                   The two methods present a new class of tools for exploring
                   the applications of noisy intermediate scale quantum
                   computers [3] to machine learning.",
  month         =  apr,
  year          =  2018,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "1804.11326"
}
@ARTICLE{Vaswani2017-lk,
  title         = "Attention is all you need",
  author        = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and
                   Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and
                   Kaiser, Lukasz and Polosukhin, Illia",
  abstract      = "The dominant sequence transduction models are based on
                   complex recurrent or convolutional neural networks in an
                   encoder-decoder configuration. The best performing models
                   also connect the encoder and decoder through an attention
                   mechanism. We propose a new simple network architecture, the
                   Transformer, based solely on attention mechanisms,
                   dispensing with recurrence and convolutions entirely.
                   Experiments on two machine translation tasks show these
                   models to be superior in quality while being more
                   parallelizable and requiring significantly less time to
                   train. Our model achieves 28.4 BLEU on the WMT 2014
                   English-to-German translation task, improving over the
                   existing best results, including ensembles by over 2 BLEU.
                   On the WMT 2014 English-to-French translation task, our
                   model establishes a new single-model state-of-the-art BLEU
                   score of 41.8 after training for 3.5 days on eight GPUs, a
                   small fraction of the training costs of the best models from
                   the literature. We show that the Transformer generalizes
                   well to other tasks by applying it successfully to English
                   constituency parsing both with large and limited training
                   data.",
  month         =  jun,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1706.03762"
}
@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}
@online{lecture_6,
title = "Lecture 6 — February 9 6.1 Recap 6.2 Kernel Ridge Regression.",
url = "https://people.eecs.berkeley.edu/~wainwrig/stat241b/lec6.pdf"}