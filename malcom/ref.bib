%% 1
@article{DBLP:journals/corr/McMahanMRA16,
	author    = {H. Brendan McMahan and
	Eider Moore and
	Daniel Ramage and
	Blaise Ag{\"{u}}era y Arcas},
	title     = {Federated Learning of Deep Networks using Model Averaging},
	journal   = {CoRR},
	volume    = {abs/1602.05629},
	year      = {2016},
	url       = {http://arxiv.org/abs/1602.05629},
	eprinttype = {arXiv},
	eprint    = {1602.05629},
	timestamp = {Mon, 13 Aug 2018 16:48:01 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/McMahanMRA16.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

%% 2
@InProceedings{pmlr-v54-mcmahan17a,
	title = 	 {{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
	author = 	 {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
	booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {1273--1282},
	year = 	 {2017},
	editor = 	 {Singh, Aarti and Zhu, Jerry},
	volume = 	 {54},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {20--22 Apr},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf},
	url = 	 {https://proceedings.mlr.press/v54/mcmahan17a.html},
	abstract = 	 {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent. }
}

%% 3
@article{DBLP:journals/corr/abs-2003-08082,
	author    = {Tzu{-}Ming Harry Hsu and
	Hang Qi and
	Matthew Brown},
	title     = {Federated Visual Classification with Real-World Data Distribution},
	journal   = {CoRR},
	volume    = {abs/2003.08082},
	year      = {2020},
	url       = {https://arxiv.org/abs/2003.08082},
	eprinttype = {arXiv},
	eprint    = {2003.08082},
	timestamp = {Wed, 01 Apr 2020 15:30:17 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2003-08082.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

%% 4
@ARTICLE{726791,
	author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	journal={Proceedings of the IEEE}, 
	title={Gradient-based learning applied to document recognition}, 
	year={1998},
	volume={86},
	number={11},
	pages={2278-2324},	
	doi={10.1109/5.726791}
}

%% 10
@article{DBLP:journals/corr/abs-1812-06127,
	author    = {Anit Kumar Sahu and
	Tian Li and
	Maziar Sanjabi and
	Manzil Zaheer and
	Ameet Talwalkar and
	Virginia Smith},
	title     = {On the Convergence of Federated Optimization in Heterogeneous Networks},
	journal   = {CoRR},
	volume    = {abs/1812.06127},
	year      = {2018},
	url       = {http://arxiv.org/abs/1812.06127},
	eprinttype = {arXiv},
	eprint    = {1812.06127},
	timestamp = {Wed, 23 Dec 2020 09:35:18 +0100},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1812-06127.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

%% 11
@article{DBLP:journals/corr/abs-1909-06335,
	author    = {Tzu{-}Ming Harry Hsu and
	Hang Qi and
	Matthew Brown},
	title     = {Measuring the Effects of Non-Identical Data Distribution for Federated
	Visual Classification},
	journal   = {CoRR},
	volume    = {abs/1909.06335},
	year      = {2019},
	url       = {http://arxiv.org/abs/1909.06335},
	eprinttype = {arXiv},
	eprint    = {1909.06335},
	timestamp = {Wed, 01 Apr 2020 15:30:17 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1909-06335.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

%% 12
@article{DBLP:journals/corr/abs-2002-07948,
	author    = {Alireza Fallah and
	Aryan Mokhtari and
	Asuman E. Ozdaglar},
	title     = {Personalized Federated Learning: {A} Meta-Learning Approach},
	journal   = {CoRR},
	volume    = {abs/2002.07948},
	year      = {2020},
	url       = {https://arxiv.org/abs/2002.07948},
	eprinttype = {arXiv},
	eprint    = {2002.07948},
	timestamp = {Thu, 16 Apr 2020 08:14:42 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-2002-07948.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

%% 14
@InProceedings{pmlr-v119-karimireddy20a,
	title = 	 {{SCAFFOLD}: Stochastic Controlled Averaging for Federated Learning},
	author =       {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
	booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
	pages = 	 {5132--5143},
	year = 	 {2020},
	editor = 	 {III, Hal Daumé and Singh, Aarti},
	volume = 	 {119},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {13--18 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v119/karimireddy20a/karimireddy20a.pdf},
	url = 	 {https://proceedings.mlr.press/v119/karimireddy20a.html},
	abstract = 	 {Federated learning is a key scenario in modern large-scale machine learning where the data remains distributed over a large number of clients and the task is to learn a centralized model without transmitting the client data. The standard optimization algorithm used in this setting is Federated Averaging (FedAvg) due to its low communication cost. We obtain a tight characterization of the convergence of FedAvg and prove that heterogeneity (non-iid-ness) in the client’s data results in a ‘drift’ in the local updates resulting in poor performance. As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the ‘client drift’. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client’s data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.}
}

%% 18
@InProceedings{pmlr-v108-reisizadeh20a,
	title = 	 {FedPAQ: A Communication-Efficient Federated Learning Method with Periodic Averaging and Quantization},
	author =       {Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Jadbabaie, Ali and Pedarsani, Ramtin},
	booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
	pages = 	 {2021--2031},
	year = 	 {2020},
	editor = 	 {Chiappa, Silvia and Calandra, Roberto},
	volume = 	 {108},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {26--28 Aug},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v108/reisizadeh20a/reisizadeh20a.pdf},
	url = 	 {https://proceedings.mlr.press/v108/reisizadeh20a.html},
	abstract = 	 {Federated learning is a distributed framework according to which  a model is trained over a set of devices, while keeping data localized. This framework  faces several systems-oriented challenges which include (i) communication bottleneck since a large number of devices upload their local updates to a parameter server, and (ii) scalability as the federated network consists of millions of devices. Due to these systems challenges as well as issues related to statistical heterogeneity of data and privacy concerns, designing a provably efficient federated learning method is of significant importance yet it remains challenging. In this paper, we present FedPAQ, a communication-efficient Federated Learning method with Periodic Averaging and Quantization. FedPAQ relies on three key features: (1) periodic averaging where models are updated locally at devices and only periodically averaged at the server; (2) partial device participation where only a fraction of devices participate in each round of the training; and (3) quantized message-passing where the edge nodes quantize their updates before uploading to the parameter server. These features address the communications and scalability challenges in federated learning. We also show that FedPAQ achieves near-optimal theoretical guarantees for strongly convex and non-convex loss functions and empirically demonstrate the communication-computation tradeoff provided by our method.}
}

%% GABRIELE
@article{ZHANG2021106775,
	title = {A survey on federated learning},
	journal = {Knowledge-Based Systems},
	volume = {216},
	pages = {106775},
	year = {2021},
	issn = {0950-7051},
	doi = {https://doi.org/10.1016/j.knosys.2021.106775},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121000381},
	author = {Chen Zhang and Yu Xie and Hang Bai and Bin Yu and Weihong Li and Yuan Gao},
	keywords = {Federated learning, Privacy protection, Machine learning},
	abstract = {Federated learning is a set-up in which multiple clients collaborate to solve machine learning problems, which is under the coordination of a central aggregator. This setting also allows the training data decentralized to ensure the data privacy of each device. Federated learning adheres to two major ideas: local computing and model transmission, which reduces some systematic privacy risks and costs brought by traditional centralized machine learning methods. The original data of the client is stored locally and cannot be exchanged or migrated. With the application of federated learning, each device uses local data for local training, then uploads the model to the server for aggregation, and finally the server sends the model update to the participants to achieve the learning goal. To provide a comprehensive survey and facilitate the potential research of this area, we systematically introduce the existing works of federated learning from five aspects: data partitioning, privacy mechanism, machine learning model, communication architecture and systems heterogeneity. Then, we sort out the current challenges and future research directions of federated learning. Finally, we summarize the characteristics of existing federated learning, and analyze the current practical application of federated learning.}
}

@INPROCEEDINGS{9155494,  author={Wang, Hao and Kaplan, Zakhary and Niu, Di and Li, Baochun},  booktitle={IEEE INFOCOM 2020 - IEEE Conference on Computer Communications},   title={Optimizing Federated Learning on Non-IID Data with Reinforcement Learning},   year={2020},  volume={},  number={},  pages={1698-1707},  doi={10.1109/INFOCOM41043.2020.9155494}}

@article{DBLP:journals/corr/abs-1806-00582,
  author    = {Yue Zhao and
               Meng Li and
               Liangzhen Lai and
               Naveen Suda and
               Damon Civin and
               Vikas Chandra},
  title     = {Federated Learning with Non-IID Data},
  journal   = {CoRR},
  volume    = {abs/1806.00582},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00582},
  eprinttype = {arXiv},
  eprint    = {1806.00582},
  timestamp = {Thu, 14 Oct 2021 09:14:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00582.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2008-06217,
  author    = {Lixu Wang and
               Shichao Xu and
               Xiao Wang and
               Qi Zhu},
  title     = {Towards Class Imbalance in Federated Learning},
  journal   = {CoRR},
  volume    = {abs/2008.06217},
  year      = {2020},
  url       = {https://arxiv.org/abs/2008.06217},
  eprinttype = {arXiv},
  eprint    = {2008.06217},
  timestamp = {Fri, 21 Aug 2020 15:05:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2008-06217.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1903-02891,
  author    = {Felix Sattler and
               Simon Wiedemann and
               Klaus{-}Robert M{\"{u}}ller and
               Wojciech Samek},
  title     = {Robust and Communication-Efficient Federated Learning from Non-IID
               Data},
  journal   = {CoRR},
  volume    = {abs/1903.02891},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.02891},
  eprinttype = {arXiv},
  eprint    = {1903.02891},
  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1903-02891.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}